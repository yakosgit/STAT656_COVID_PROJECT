---
title: 'STAT656_COVID_Project'
output:
  html_document: default
---

# Loading packages and Data
```{r loadingPackages}
packs = c('dplyr','ggplot2', 'caret','corrplot', 'e1071','readr', 'pROC')
lapply(packs,require,character.only=TRUE)
```

```{r loadingData}
rm(list=ls())# Just to clean up the memory again

# original data from (https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data-with-Ge/n8mc-b4w4) is ~85+ million rows and ~13GB file size

# filtered data (https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data-with-Ge/n8mc-b4w4/data) is ~1.4+ million rows and ~200MB file size - removing (missing, NA, unknown) from the following columns:
# death_yn, age_group, hosp_yn,  underlying_conditions_yn, 

# "dataSetPath" is the path to the directory on your PC where you  place the dataset csv file 
dataSetPath = '/home/yako/Desktop/TAMU_STAT/STAT656_Applied Analytics/STAT656FinalProject/Datasets'
dataSetName = 'COVID-19_Case_Surveillance_Public_Use_Data_with_Geography_Filterd.csv'
dataSet     = read_csv(file.path(dataSetPath,dataSetName))
```
# Exploratory analysis
items that usually need to be checked:

* data structures
* checking for missing data
* Converting qualitative features to dummy variables
* extreme observations
* skewness/transformations
* correlations

```{r Check data structures}
str(dataSet)
```

```{r Check if any features have NA}
sapply(dataSet, function(x) sum(is.na(x)))

```

```{r Check number of unique values}
rbind(sapply(dataSet,function(x){ length(unique(x))}),
      sapply(dataSet,class))
```

```{r plot hist}

```

```{r Data structure and spliting}
Y       = select(dataSet,death_yn) %>% unlist()
X       = select(dataSet,-death_yn)
```

# Building Models:

```{r}

```

```{r}

```

```{r}

```

```{r}

```


